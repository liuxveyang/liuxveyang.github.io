<!DOCTYPE html>
<html lang="zh-cn">
	<head>
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="author" content="liuxueyang">
<meta name="description" content="舊博客">
<meta name="generator" content="Hugo 0.48" />
<title>重新整理博客</title>
<link rel="shortcut icon" href="https://liuxveyang.github.io/images/favicon.ico">
<link rel="stylesheet" href="https://liuxveyang.github.io/css/style.css">
<link rel="stylesheet" href="https://liuxveyang.github.io/css/highlight.css">



<link rel="stylesheet" href="https://liuxveyang.github.io/css/monosocialiconsfont.css">



<link href="https://liuxveyang.github.io/index.xml" rel="alternate" type="application/rss+xml" title="LXY Site" />


<meta property="og:title" content="重新整理博客" />
<meta property="og:description" content="这么长时间过去了，好像很久不管这个博客了，差不多10个月过去了。是时候 把这些零散的东西整理一下了。争取把之前在别的地方写的东西都整理到这里 来。这样以后查看会方便很多吧！
主要是2014年的「博客园」上面的东西。找了一下现有的工具hexo-migrator-cnblogs发现早已经不维护了，现在也不能用了。所以写了一个简单的脚本来爬取我的博客：
# 2017/04/11 00:45:15 AM # Author: liuxueyang from bs4 import BeautifulSoup import requests import re import os." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://liuxveyang.github.io/posts/%E9%87%8D%E6%96%B0%E6%95%B4%E7%90%86%E5%8D%9A%E5%AE%A2/" /><meta property="article:published_time" content="2017-04-10T21:11:11&#43;00:00"/>
<meta property="article:modified_time" content="2017-04-10T21:11:11&#43;00:00"/>


<meta itemprop="name" content="重新整理博客">
<meta itemprop="description" content="这么长时间过去了，好像很久不管这个博客了，差不多10个月过去了。是时候 把这些零散的东西整理一下了。争取把之前在别的地方写的东西都整理到这里 来。这样以后查看会方便很多吧！
主要是2014年的「博客园」上面的东西。找了一下现有的工具hexo-migrator-cnblogs发现早已经不维护了，现在也不能用了。所以写了一个简单的脚本来爬取我的博客：
# 2017/04/11 00:45:15 AM # Author: liuxueyang from bs4 import BeautifulSoup import requests import re import os.">


<meta itemprop="datePublished" content="2017-04-10T21:11:11&#43;00:00" />
<meta itemprop="dateModified" content="2017-04-10T21:11:11&#43;00:00" />
<meta itemprop="wordCount" content="262">



<meta itemprop="keywords" content="" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="重新整理博客"/>
<meta name="twitter:description" content="这么长时间过去了，好像很久不管这个博客了，差不多10个月过去了。是时候 把这些零散的东西整理一下了。争取把之前在别的地方写的东西都整理到这里 来。这样以后查看会方便很多吧！
主要是2014年的「博客园」上面的东西。找了一下现有的工具hexo-migrator-cnblogs发现早已经不维护了，现在也不能用了。所以写了一个简单的脚本来爬取我的博客：
# 2017/04/11 00:45:15 AM # Author: liuxueyang from bs4 import BeautifulSoup import requests import re import os."/>


    </head>
<body>
    <nav class="main-nav">
	
		<a href='https://liuxveyang.github.io'> <span class="arrow">←</span>Home</a>
	

	

	
		<a class="cta" href="https://liuxveyang.github.io/index.xml">Subscribe</a>
	
</nav>

    <section id="wrapper">
        
        
<article class="post">
    <header>
        <h1>重新整理博客</h1>
        <h2 class="subtitle"></h2>
        <h2 class="headline">
        April 10, 2017
        <br>
        
        </h2>
    </header>
    <section id="post-body">
        <p>这么长时间过去了，好像很久不管这个博客了，差不多10个月过去了。是时候
把这些零散的东西整理一下了。争取把之前在别的地方写的东西都整理到这里
来。这样以后查看会方便很多吧！</p>

<p>主要是2014年的「博客园」上面的东西。找了一下现有的工具<a href="https://npm.taobao.org/package/hexo-migrator-cnblogs">hexo-migrator-cnblogs</a>发现早已经不维护了，现在也不能用了。所以写了一个简单的脚本来爬取我的博客：</p>

<pre><code class="language-python"># 2017/04/11 00:45:15 AM
# Author: liuxueyang

from bs4 import BeautifulSoup
import requests
import re
import os.path

url = 'http://www.cnblogs.com/liuxueyang/default.html?page='
page_nums = range(1, 11)
cnt = 0
already_urls = []

if os.path.exists('already.txt'):
    with open('already.txt', 'r') as already_f:
        already_urls = already_urls + already_f.readlines()

for page_num in page_nums:
    url1 = url + str(page_num)
    r = requests.get(url1)
    soup = BeautifulSoup(r.content, 'html5lib')

    titles = soup.find_all('a', class_='posttitle')
    for title in titles:
        print '==' * 20, '\n\n'
        cnt += 1
        blog_url = title.get('href')
        blog_name = title.string
        blog_r = requests.get(blog_url)

        if blog_url + '\n' in already_urls:
            print cnt, blog_name
            continue

        blog_soup = BeautifulSoup(blog_r.content, 'html5lib')
        blog_body = blog_soup.find(id='cnblogs_post_body')
        blog_date = blog_soup.find(id='post-date').text

        print cnt, blog_url
        tags = 'tags: \n'

        body = [
            '---\n', 'title: &quot;%s&quot;\n' % blog_name, 'date: %s\n' % blog_date,
            tags, '---\n\n'
        ]

        for child in blog_body.children:
            if child.name == 'p':
                body.append(child.text + '\n')
            elif child.name == 'div' and child.has_attr('class') and child.get(
                    'class')[0] == 'cnblogs_code':
                lines = child.text.split('\n')
                body.append('\n```cpp\n')
                for line in lines:
                    line = re.sub(r'^ *', '', line)
                    line = re.sub(r'^\d* ', '', line)
                    body.append(line + '\n')
                body.append('```\n\n')

        for line in body:
            print line,
        file_name = re.sub(r' |/', '-', blog_name) + '.md'
        print file_name

        action = raw_input()

        if action == 'S':
            process_later_file = 'later.txt'
            f = open(process_later_file, 'a')
            f.write(blog_url + '\n')
            print '-' * 30
            print 'process by hand later!'
            print '-' * 30
            f.close()
            continue

        if action:
            action = action.split(',')
            tags = 'tags: [' + ', '.join(action) + ']' + '\n'
            body[3] = tags

        ff = open(file_name, 'w')
        for b in body:
            ff.write(b.encode('utf-8'))
        ff.close()

        with open('already.txt', 'a') as already_f:
            already_f.write(blog_url + '\n')

</code></pre>

<p>没有加处理图片的功能。因为以前的博客用的图片并不多，手动处理。它会抓取博客正文和嵌入的代码，处理嵌入代码的时候花了比较多的时间，比如如果从xml中获取到嵌入的代码，然后去掉行号之类的。这个程序抓取到一篇博客就显示出博客正文和代码块来，等待用户输入，如果正常，之间按回车处理下一篇，如果原来的博客有图片或者抓取的内容不完整，按下<code>S</code>会把当前博客的地址保存到一个<code>later.txt</code>这个文本文件里，随后手动处理。已经处理好的博客地址放到<code>already.txt</code>文件里，这样避免了重复处理。结果还可以。</p>

<p>总共有455篇日志了。可能是我的电脑太老的原因，hexo生成的时候竟然用了好几分钟。所以之后想试一试Hakyll了。</p>

    </section>
</article>

<footer id="post-meta" class="clearfix">
    
    <img class="avatar" src="https://liuxveyang.github.io/images/avatar.png">
    <div>
        <span class="dark">liuxueyang</span>
        <span>I am a learner. :P</span>
    </div>
    
    <section id="sharing">
        <a class="twitter" href="https://twitter.com/intent/tweet?text=https%3a%2f%2fliuxveyang.github.io%2fposts%2f%25E9%2587%258D%25E6%2596%25B0%25E6%2595%25B4%25E7%2590%2586%25E5%258D%259A%25E5%25AE%25A2%2f - %e9%87%8d%e6%96%b0%e6%95%b4%e7%90%86%e5%8d%9a%e5%ae%a2 "><span class="icon-twitter"> tweet</span></a>

<a class="facebook" href="#" onclick="
    window.open(
      'https://www.facebook.com/sharer/sharer.php?u='+encodeURIComponent(location.href),
      'facebook-share-dialog',
      'width=626,height=436');
    return false;"><span class="icon-facebook-rect"> Share</span>
</a>

    </section>
</footer>

<div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "liuxveyang" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

<ul id="post-list" class="archive readmore">
    <h3>Read more</h3>

    
    
    
        <li>
            <a href="/posts/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B06%E6%8A%80%E6%9C%AF%E7%B1%BB/">读书笔记6「技术类」<aside class="dates">May 14 2018</aside></a>
        </li>
    
        <li>
            <a href="/posts/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B05/">读书笔记5<aside class="dates">Apr 29 2018</aside></a>
        </li>
    
        <li>
            <a href="/posts/%E6%95%B4%E7%90%86%E5%8D%9A%E5%AE%A2/">整理博客<aside class="dates">Apr 29 2018</aside></a>
        </li>
    
        <li>
            <a href="/posts/2017-12-23-summary-for-2017/">2017 年结束了<aside class="dates">Dec 23 2017</aside></a>
        </li>
    
        <li>
            <a href="/posts/clustering/">Clustering<aside class="dates">May 17 2017</aside></a>
        </li>
    
        <li>
            <a href="/posts/building-roads-to-connect-cities/">Building Roads to Connect Cities<aside class="dates">May 17 2017</aside></a>
        </li>
    
        <li>
            <a href="/posts/detecting-anomalies-in-currency-exchange-rates/">Detecting Anomalies in Currency Exchange Rates<aside class="dates">May 17 2017</aside></a>
        </li>
    
        <li>
            <a href="/posts/computing-minimum-cost-of-a-flight/">Computing the Minimum Cost of a Flight<aside class="dates">May 17 2017</aside></a>
        </li>
    
        <li>
            <a href="/posts/checking-whether-a-graph-is-bipartite/">Checking whether a Graph is Bipartite<aside class="dates">May 17 2017</aside></a>
        </li>
    
        <li>
            <a href="/posts/computing-minimum-number-of-flight-segments/">Computing the Minimum Number of Flight Segments<aside class="dates">May 16 2017</aside></a>
        </li>
    
</ul>



        <footer id="footer">
    
        <div id="social">

	
	
    
    <a class="symbol" href="https://www.github.com/liuxueyang">
        circlegithub
    </a>
    


</div>

    
    <p class="small">
    
        © Copyright 2018 liuxueyang
    
    </p>
</footer>

    </section>
    <script src="//ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script>
<script src="https://liuxveyang.github.io/js/main.js"></script>
<script src="https://liuxveyang.github.io/js/highlight.js"></script>
<script>hljs.initHighlightingOnLoad();</script>




<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-125224924-1', 'auto');
	
	ga('send', 'pageview');
}
</script>


</body>
</html>
